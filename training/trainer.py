import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from model.g_bottomup import gBottomup
from model.tcn_autoencoder import TCNAutoEncoder
from training.datapair import PairDataset
from training.losses import CombinedLoss
from typing import Dict, Any, List
import numpy as np
import os

class Trainer:
    """
    gBottomup íƒìƒ‰ê³¼ TCN-AutoEncoder í•™ìŠµì„ 'ë§¤ ìŠ¤í…'ë§ˆë‹¤ ì—°ê²°í•˜ì—¬
    ë‹¨ê³„ë³„ ì ì‘í˜• í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, model: TCNAutoEncoder, all_data: np.ndarray, config: Dict[str, Any]):
        """
        Args:
            model (TCNAutoEncoder): í•™ìŠµì‹œí‚¬ TCN-AutoEncoder ëª¨ë¸.
            all_data (np.ndarray): ì „ì²´ ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°.
            config (Dict[str, Any]): í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬.
        """
        self.model = model
        self.all_data = all_data
        self.config = config
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)

        # gBottomup ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ˆê¸° ì¸ì½”ë”ëŠ” í•™ìŠµë˜ì§€ ì•Šì€ ìƒíƒœ)
        self.g_bottomup = gBottomup(encoder=self.model.encoder, config=config['gbottomup'])
        
        # í•™ìŠµ ê´€ë ¨ ì„¤ì •
        train_config = config['training']
        self.optimizer = optim.Adam(self.model.parameters(), lr=train_config.get('learning_rate', 1e-4))
        self.loss_fn = CombinedLoss(
            lambda_recon=train_config.get('lambda_recon', 0.5),
            temperature=train_config.get('temperature', 0.1)
        ).to(self.device)
        self.train_epochs = train_config.get('train_epochs', 5)
        self.batch_size = train_config.get('batch_size', 64)

        # ê¸ì • ìŒì„ ëˆ„ì í•˜ì—¬ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.accumulated_pairs: List[List[List[int]]] = []

    def _train_on_accumulated_pairs(self):
        """ëˆ„ì ëœ ì „ì²´ ê¸ì • ìŒìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµ(ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •)í•©ë‹ˆë‹¤."""
        if not self.accumulated_pairs:
            print("No positive pairs to train on. Skipping model update.")
            return

        print(f"\n--- Training Step ---")
        print(f"Updating the model with {len(self.accumulated_pairs)} accumulated positive pairs...")
        
        dataset = PairDataset(
            self.accumulated_pairs, 
            self.all_data, 
            max_len=self.config['model'].get('max_len', 100)
        )
        # ë°ì´í„°ê°€ ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ì‘ì„ ê²½ìš° ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì ˆ
        effective_batch_size = min(self.batch_size, len(dataset))
        if effective_batch_size == 0: return
        
        dataloader = DataLoader(dataset, batch_size=effective_batch_size, shuffle=True)
        
        self.model.train()
        for epoch in range(self.train_epochs):
            total_loss = 0
            for batch_hist, batch_fut in dataloader:
                batch_hist, batch_fut = batch_hist.to(self.device), batch_fut.to(self.device)
                
                self.optimizer.zero_grad()
                
                z_hist, recon_hist = self.model(batch_hist)
                z_fut, recon_fut = self.model(batch_fut)
                
                loss = self.loss_fn(z_hist, recon_hist, batch_hist, z_fut, recon_fut, batch_fut)
                
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch+1}/{self.train_epochs}, Average Loss: {avg_loss:.4f}")
        print(f"--- Model Updated ---")


    def run(self):
        """ë‹¨ê³„ë³„ ì ì‘í˜• í•™ìŠµ ë° íƒìƒ‰ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        print("Starting bootstrapped adaptive learning process...")
        
        # gBottomup.fit()ì€ ì œë„ˆë ˆì´í„°ì´ë¯€ë¡œ, for loopë¡œ ê° ìŠ¤í…ì„ ìˆœíšŒí•©ë‹ˆë‹¤.
        g_bottomup_generator = self.g_bottomup.fit(self.all_data)
        
        step_count = 0
        for positive_pairs_this_step in g_bottomup_generator:
            step_count += 1
            is_first_step = (step_count == 1)
            
            print(f"\n[Step {step_count}] gBottomup provided {len(positive_pairs_this_step)} new positive pairs.")
            if is_first_step:
                print("This is the first step, based on raw data statistics.")
            else:
                print("Based on the latest embedding space statistics.")

            # 1. ì´ë²ˆ ìŠ¤í…ì—ì„œ ì–»ì€ ê¸ì • ìŒì„ ëˆ„ì  ë°ì´í„°ì…‹ì— ì¶”ê°€
            self.accumulated_pairs.extend(positive_pairs_this_step)

            # 2. ëˆ„ì ëœ ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸
            self._train_on_accumulated_pairs()
            
            # 3. ğŸ”¥ gBottomupì— ì—…ë°ì´íŠ¸ëœ ì¸ì½”ë”ë¥¼ ë‹¤ì‹œ ì£¼ì… (send)
            #    ì œë„ˆë ˆì´í„°ì˜ ë‹¤ìŒ ë£¨í”„(ë‹¤ìŒ ìŠ¤í…)ëŠ” ì´ ìƒˆë¡œìš´ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
            try:
                g_bottomup_generator.send(self.model.encoder)
            except StopIteration:
                # gBottomupì´ ë§ˆì§€ë§‰ ìŠ¤í…ì´ì—ˆë˜ ê²½ìš°, sendì—ì„œ StopIterationì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
                break

        print("\n========================================================")
        print("gBottomup process finished.")
        print("Adaptive learning and detection process complete.")
        print("========================================================")