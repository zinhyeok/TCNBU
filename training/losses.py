import torch
import torch.nn as nn
import torch.nn.functional as F

def info_nce_loss_with_filtering(z_history: torch.Tensor, z_future: torch.Tensor, temperature: float = 0.1, similarity_threshold: float = 0.9):
    """
    ë°°ì¹˜ ë‚´ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ê³¼ ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§ì„ ì ìš©í•œ InfoNCE ì†ì‹¤ í•¨ìˆ˜.

    Args:
        z_history (torch.Tensor): History ì„¸ê·¸ë¨¼íŠ¸ì˜ ìš”ì•½ ì„ë² ë”© ë²¡í„° (Batch, embedding_dim).
        z_future (torch.Tensor): Future ì„¸ê·¸ë¨¼íŠ¸ì˜ ìš”ì•½ ì„ë² ë”© ë²¡í„° (Batch, embedding_dim).
        temperature (float): ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ì˜¨ë„ íŒŒë¼ë¯¸í„°.
        similarity_threshold (float): 'ê°€ì§œ ë¶€ì •'ì„ í•„í„°ë§í•˜ê¸° ìœ„í•œ ìœ ì‚¬ë„ ì„ê³„ê°’.

    Returns:
        torch.Tensor: ê³„ì‚°ëœ InfoNCE ì†ì‹¤ ê°’ (ìŠ¤ì¹¼ë¼).
    """
    device = z_history.device
    batch_size = z_history.size(0)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    # (B, 1, D)ì™€ (1, B, D) -> (B, B)
    sim_matrix = F.cosine_similarity(z_history.unsqueeze(1), z_future.unsqueeze(0), dim=-1)

    # ğŸ”¥ "ê°€ì§œ ë¶€ì •" í•„í„°ë§ ë¡œì§
    # ê¸ì • ìŒ (ëŒ€ê°ì„  ìš”ì†Œ)ì˜ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì  ì„ê³„ê°’ ì„¤ì •
    positive_similarities = torch.diag(sim_matrix)
    dynamic_threshold = positive_similarities.mean()
    
    # ìê¸° ìì‹ ì„ ì œì™¸í•œ (off-diagonal) ë¶€ì • ìŒ í›„ë³´ë“¤ ì¤‘ì—ì„œ
    # ì„ê³„ê°’ì„ ë„˜ëŠ” 'ê°€ì§œ ë¶€ì •'ì„ ì‹ë³„
    mask = torch.ones_like(sim_matrix, dtype=torch.bool, device=device)
    mask.fill_diagonal_(False) # ìê¸° ìì‹ (ê¸ì • ìŒ)ì€ í•„í„°ë§ ëŒ€ìƒì—ì„œ ì œì™¸
    
    false_negatives = (sim_matrix > dynamic_threshold) & mask
    
    # ìœ ì‚¬ë„ í–‰ë ¬ì—ì„œ 'ê°€ì§œ ë¶€ì •' ìœ„ì¹˜ì˜ ê°’ì„ ë§¤ìš° ë‚®ê²Œ ì„¤ì •í•˜ì—¬ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
    sim_matrix.masked_fill_(false_negatives, -1e9)
    
    # ìµœì¢… ì†ì‹¤ ê³„ì‚°
    sim_matrix /= temperature
    labels = torch.arange(batch_size, device=device)
    
    loss = F.cross_entropy(sim_matrix, labels)
    return loss


class CombinedLoss(nn.Module):
    """
    ëŒ€ì¡° í•™ìŠµ ì†ì‹¤ê³¼ ì¬êµ¬ì„± ì†ì‹¤ì„ ê²°í•©í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, lambda_recon: float = 0.5, temperature: float = 0.1):
        """
        Args:
            lambda_recon (float): ì¬êµ¬ì„± ì†ì‹¤ì˜ ê°€ì¤‘ì¹˜.
            temperature (float): InfoNCE ì†ì‹¤ì˜ ì˜¨ë„ íŒŒë¼ë¯¸í„°.
        """
        super().__init__()
        self.lambda_recon = lambda_recon
        self.temperature = temperature
        self.reconstruction_loss_fn = nn.MSELoss()

    def forward(self, 
                z_sequence_hist: torch.Tensor, x_recon_hist: torch.Tensor, x_original_hist: torch.Tensor,
                z_sequence_fut: torch.Tensor, x_recon_fut: torch.Tensor, x_original_fut: torch.Tensor
               ) -> torch.Tensor:
        """
        Args:
            z_sequence_hist, z_sequence_fut: ì¸ì½”ë”ê°€ ì¶œë ¥í•œ í¬ì¸íŠ¸ë³„ ì„ë² ë”© ì‹œí€€ìŠ¤.
            x_recon_hist, x_recon_fut: ë””ì½”ë”ê°€ ì¶œë ¥í•œ ì¬êµ¬ì„± ì‹œí€€ìŠ¤.
            x_original_hist, x_original_fut: ì›ë³¸ ì‹œí€€ìŠ¤.

        Returns:
            torch.Tensor: ìµœì¢… ê²°í•© ì†ì‹¤ ê°’.
        """
        # 1. ì¬êµ¬ì„± ì†ì‹¤ ê³„ì‚°
        loss_recon_hist = self.reconstruction_loss_fn(x_recon_hist, x_original_hist)
        loss_recon_fut = self.reconstruction_loss_fn(x_recon_fut, x_original_fut)
        total_loss_recon = loss_recon_hist + loss_recon_fut

        # 2. ëŒ€ì¡° í•™ìŠµ ì†ì‹¤ ê³„ì‚°
        # í¬ì¸íŠ¸ë³„ ì„ë² ë”© ì‹œí€€ìŠ¤ì— Temporal Poolingì„ ì ìš©í•˜ì—¬ ìš”ì•½ ë²¡í„° ìƒì„±
        z_hist_summary = z_sequence_hist.mean(dim=1)
        z_fut_summary = z_sequence_fut.mean(dim=1)
        
        loss_contrastive = info_nce_loss_with_filtering(
            z_hist_summary, z_fut_summary, self.temperature
        )
        
        # 3. ë‘ ì†ì‹¤ì„ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ì†ì‹¤ ë°˜í™˜
        final_loss = loss_contrastive + self.lambda_recon * total_loss_recon
        return final_loss